#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†è„šæœ¬ - ä¸­æ–‡â†’è‹±æ–‡ç¿»è¯‘
ä» JSONL æ ¼å¼è½¬æ¢ä¸º ChineseNMT æ‰€éœ€æ ¼å¼
ä½¿ç”¨ Jieba å¤„ç†ä¸­æ–‡ï¼ŒSentencePiece å¤„ç†è‹±æ–‡

Usage:
    python preprocess_data.py
"""

import json
import os
import re
import pickle
from collections import Counter
from tqdm import tqdm
import jieba
import sentencepiece as spm
import config  # Import config module


class DataPreprocessor:
    """æ•°æ®é¢„å¤„ç†å™¨ - ä¸­æ–‡â†’è‹±æ–‡"""

    def __init__(self,
                 source_data_dir="/mnt/c/Users/sysu/Desktop/nlp_hw3/dataset_zh_en/AP0004_Midterm&Final_translation_dataset_zh_en",
                 train_size="10k"):
        """
        Args:
            source_data_dir: æºæ•°æ®ç›®å½•
            train_size: è®­ç»ƒé›†å¤§å°ï¼Œå¯é€‰ "10k" æˆ– "100k"
        """
        self.source_data_dir = source_data_dir
        self.train_size = train_size

        # æ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨è®¾ç½®è¾“å‡ºç›®å½•
        self.output_dir = f"./data_{train_size}"
        self.tokenizer_dir = f"./data_{train_size}/tokenizer"

        self.makedirs()

    def makedirs(self):
        """åˆ›å»ºå¿…è¦çš„ç›®å½•"""
        os.makedirs(self.output_dir, exist_ok=True)
        os.makedirs(os.path.join(self.output_dir, 'json'), exist_ok=True)
        os.makedirs(self.tokenizer_dir, exist_ok=True)

    def clean_text(self, text, lang='en'):
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        # ç§»é™¤æ§åˆ¶å­—ç¬¦
        text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)

        # æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
        if lang == 'en':
            text = re.sub(r'\s+', ' ', text)
        else:  # ä¸­æ–‡
            text = re.sub(r'\s+', '', text)

        return text.strip()

    def load_jsonl(self, file_path):
        """åŠ è½½JSONLæ–‡ä»¶"""
        data = []
        print(f"Loading {file_path}...")
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                data.append(json.loads(line.strip()))
        return data

    def clean_dataset(self, data, max_len=200, min_len=3):
        """æ¸…ç†æ•´ä¸ªæ•°æ®é›†"""
        cleaned = []
        for item in tqdm(data, desc="Cleaning dataset"):
            zh = self.clean_text(item['zh'], 'zh')
            en = self.clean_text(item['en'], 'en')

            # æ£€æŸ¥é•¿åº¦é™åˆ¶
            if min_len <= len(zh) <= max_len and min_len <= len(en) <= max_len:
                # æ³¨æ„ï¼š[ä¸­æ–‡, è‹±æ–‡] - ä¸­æ–‡ä½œä¸ºæºè¯­è¨€ï¼Œè‹±æ–‡ä½œä¸ºç›®æ ‡è¯­è¨€
                cleaned.append([zh, en])

        return cleaned

    def save_json(self, data, file_path):
        """ä¿å­˜ä¸ºJSONæ ¼å¼"""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"Saved to {file_path}")

    def build_chinese_vocab_with_jieba(self, texts, min_freq=2):
        """ä½¿ç”¨Jiebaæ„å»ºä¸­æ–‡è¯æ±‡è¡¨"""
        print("Building Chinese vocabulary with Jieba...")

        # æ”¶é›†æ‰€æœ‰ä¸­æ–‡è¯æ±‡
        all_tokens = []
        for text in tqdm(texts, desc="Tokenizing Chinese"):
            tokens = list(jieba.cut(text))
            all_tokens.extend(tokens)

        # ç»Ÿè®¡è¯é¢‘
        word_freq = Counter(all_tokens)

        # æ„å»ºè¯æ±‡è¡¨ï¼ˆåŒ…å«ç‰¹æ®Šç¬¦å·ï¼‰
        special_tokens = ['<pad>', '<unk>', '<s>', '</s>']
        vocab = special_tokens.copy()

        # æ·»åŠ é«˜é¢‘è¯
        for word, freq in word_freq.most_common():
            if freq >= min_freq and word not in vocab and word.strip():
                vocab.append(word)

        # åˆ›å»ºè¯æ±‡æ˜ å°„
        word2idx = {word: idx for idx, word in enumerate(vocab)}
        idx2word = {idx: word for word, idx in word2idx.items()}

        print(f"Chinese vocabulary size: {len(vocab)}")
        print(f"Min frequency: {min_freq}")

        # ä¿å­˜è¯æ±‡è¡¨
        vocab_info = {
            'word2idx': word2idx,
            'idx2word': idx2word,
            'word_freq': dict(word_freq),
            'min_freq': min_freq,
            'vocab_size': len(vocab)
        }

        vocab_path = os.path.join(self.tokenizer_dir, 'chinese_vocab.pkl')
        with open(vocab_path, 'wb') as f:
            pickle.dump(vocab_info, f)

        print(f"Chinese vocabulary saved to {vocab_path}")
        return vocab_info

    def train_english_sentencepiece(self, texts, vocab_size=8000):
        """ä¸ºè‹±æ–‡è®­ç»ƒSentencePiece BPEæ¨¡å‹"""
        print("\nTraining English SentencePiece model...")

        # å‡†å¤‡è®­ç»ƒæ–‡æœ¬
        temp_file = os.path.join(self.tokenizer_dir, "eng_corpus.txt")
        with open(temp_file, 'w', encoding='utf-8') as f:
            for text in texts:
                f.write(text + '\n')

        print(f"Training corpus size: {len(texts)} sentences")
        print(f"Vocabulary size: {vocab_size}")

        model_prefix = os.path.join(self.tokenizer_dir, 'eng')

        # è®­ç»ƒBPEæ¨¡å‹
        spm.SentencePieceTrainer.train(
            input=temp_file,
            model_prefix=model_prefix,
            model_type='bpe',
            vocab_size=vocab_size,
            character_coverage=1.0,
            pad_id=0,
            unk_id=1,
            bos_id=2,
            eos_id=3,
            pad_piece='<pad>',
            unk_piece='<unk>',
            bos_piece='<s>',
            eos_piece='</s>',
            split_digits=True
        )

        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_file)

        print(f"English model saved: {model_prefix}.model")
        print(f"English vocab saved: {model_prefix}.vocab")

    def process_all_data(self):
        """å¤„ç†æ‰€æœ‰æ•°æ®é›†"""
        print("="*60)
        print("Data Preprocessing for Chineseâ†’English NMT")
        print("="*60)
        print(f"Training data size: {self.train_size}")
        print("="*60)

        # æ–‡ä»¶è·¯å¾„
        train_path = os.path.join(self.source_data_dir, f"train_{self.train_size}.jsonl")
        valid_path = os.path.join(self.source_data_dir, "valid.jsonl")
        test_path = os.path.join(self.source_data_dir, "test.jsonl")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for path in [train_path, valid_path, test_path]:
            if not os.path.exists(path):
                raise FileNotFoundError(f"Data file not found: {path}")

        # åŠ è½½æ•°æ®
        print("\n[1/4] Loading data...")
        train_data = self.load_jsonl(train_path)
        valid_data = self.load_jsonl(valid_path)
        test_data = self.load_jsonl(test_path)

        print(f"Train: {len(train_data)} samples")
        print(f"Valid: {len(valid_data)} samples")
        print(f"Test: {len(test_data)} samples")

        # æ¸…ç†æ•°æ®
        print("\n[2/4] Cleaning data...")
        train_clean = self.clean_dataset(train_data)
        valid_clean = self.clean_dataset(valid_data)
        test_clean = self.clean_dataset(test_data)

        print(f"After cleaning - Train: {len(train_clean)}, Valid: {len(valid_clean)}, Test: {len(test_clean)}")

        # ä¿å­˜æ¸…ç†åçš„æ•°æ®
        print("\n[3/4] Saving cleaned data...")
        self.save_json(train_clean, os.path.join(self.output_dir, 'json', 'train.json'))
        self.save_json(valid_clean, os.path.join(self.output_dir, 'json', 'dev.json'))
        self.save_json(test_clean, os.path.join(self.output_dir, 'json', 'test.json'))

        # æå–ä¸­æ–‡å’Œè‹±æ–‡æ–‡æœ¬
        zh_texts = [item[0] for item in train_clean]  # ä¸­æ–‡ï¼ˆæºè¯­è¨€ï¼‰
        en_texts = [item[1] for item in train_clean]  # è‹±æ–‡ï¼ˆç›®æ ‡è¯­è¨€ï¼‰

        # è®­ç»ƒåˆ†è¯å™¨
        print("\n[4/4] Training tokenizers...")

        # ä¸­æ–‡ï¼šä½¿ç”¨Jieba + è¯æ±‡è¡¨
        zh_vocab_info = self.build_chinese_vocab_with_jieba(zh_texts, min_freq=config.min_freq)

        # è‹±æ–‡ï¼šä½¿ç”¨SentencePiece BPE
        self.train_english_sentencepiece(en_texts, vocab_size=8000)

        print("\n" + "="*60)
        print("Preprocessing completed successfully!")
        print("="*60)
        print(f"Data directory: {self.output_dir}")
        print(f"Tokenizer directory: {self.tokenizer_dir}")
        print(f"\nChinese (source) vocab size: {zh_vocab_info['vocab_size']}")
        print(f"English (target) vocab size: 8000")
        print(f"\nâœ… Dataset '{self.train_size}' is ready!")
        print(f"   - Update config.py: set DATA_SIZE = '{self.train_size}'")
        print(f"   - Update config.py: set src_vocab_size = {zh_vocab_info['vocab_size']}")
        print(f"\nYou can now train the model using: python main.py")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    import sys

    # æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é€‰æ‹©æ•°æ®é›†å¤§å°
    if len(sys.argv) > 1:
        train_size = sys.argv[1]
        if train_size not in ["10k", "100k"]:
            print("Error: train_size must be '10k' or '100k'")
            print("Usage: python preprocess_data.py [10k|100k]")
            sys.exit(1)
    else:
        # é»˜è®¤å¤„ç†ä¸¤ä¸ªæ•°æ®é›†
        print("="*60)
        print("Processing BOTH datasets (10k and 100k)")
        print("="*60)
        print("You can also specify one dataset:")
        print("  python preprocess_data.py 10k")
        print("  python preprocess_data.py 100k")
        print("="*60)
        print()

        # å¤„ç† 10k æ•°æ®é›†
        print("ğŸ“¦ [1/2] Processing 10k dataset...")
        preprocessor_10k = DataPreprocessor(
            source_data_dir="/mnt/c/Users/sysu/Desktop/nlp_hw3/dataset_zh_en/AP0004_Midterm&Final_translation_dataset_zh_en",
            train_size="10k"
        )
        preprocessor_10k.process_all_data()

        print("\n" + "="*60)
        print()

        # å¤„ç† 100k æ•°æ®é›†
        print("ğŸ“¦ [2/2] Processing 100k dataset...")
        preprocessor_100k = DataPreprocessor(
            source_data_dir="/mnt/c/Users/sysu/Desktop/nlp_hw3/dataset_zh_en/AP0004_Midterm&Final_translation_dataset_zh_en",
            train_size="100k"
        )
        preprocessor_100k.process_all_data()

        print("\n" + "="*80)
        print("ğŸ‰ All datasets preprocessed successfully!")
        print("="*80)
        print("\nNext steps:")
        print("1. Choose dataset in config.py: set DATA_SIZE = '10k' or '100k'")
        print("2. Update src_vocab_size in config.py based on output above")
        print("3. Run training: python main.py")
        print("="*80)
        return

    # å¤„ç†å•ä¸ªæ•°æ®é›†
    preprocessor = DataPreprocessor(
        source_data_dir="/mnt/c/Users/sysu/Desktop/nlp_hw3/dataset_zh_en/AP0004_Midterm&Final_translation_dataset_zh_en",
        train_size=train_size
    )
    preprocessor.process_all_data()


if __name__ == "__main__":
    main()
